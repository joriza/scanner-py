# Plan de OptimizaciÃ³n de Rendimiento - Scanner Pro Python

**Fecha:** 27 de enero de 2026
**Estado:** PlanificaciÃ³n
**Enfoque:** Alto impacto, baja complejidad, librerÃ­as mÃ¡s rÃ¡pidas

## Resumen Ejecutivo

Este documento detalla las optimizaciones propuestas para mejorar el rendimiento del proyecto Scanner Pro Python, priorizando cambios de alto impacto con baja complejidad y evaluando librerÃ­as alternativas mÃ¡s rÃ¡pidas.

## LibrerÃ­as Alternativas Evaluadas

### 1. Polars vs pandas

| Aspecto | pandas | Polars | RecomendaciÃ³n |
|---------|--------|--------|---------------|
| Velocidad | Baseline | 10-100x mÃ¡s rÃ¡pido | âœ… Polars |
| API | EstÃ¡ndar | Similar a pandas | âœ… Polars |
| Complejidad | - | Baja | âœ… Polars |
| Compatibilidad | - | Puede convertir a pandas | âœ… Polars |
| Mantenimiento | Estable | Activo | âœ… Polars |

**DecisiÃ³n:** Reemplazar pandas con **Polars** donde sea posible sin complicar el cÃ³digo.

### 2. TA-Lib vs pandas-ta

| Aspecto | pandas-ta | TA-Lib | RecomendaciÃ³n |
|---------|-----------|--------|---------------|
| Velocidad | Baseline | 5-20x mÃ¡s rÃ¡pido | âœ… TA-Lib |
| API | Python puro | Python wrapper sobre C | âš ï¸ TA-Lib |
| InstalaciÃ³n | pip simple | Requiere compilaciÃ³n | âš ï¸ TA-Lib |
| Complejidad | Baja | Media | âš ï¸ TA-Lib |

**DecisiÃ³n:** Mantener **pandas-ta** por simplicidad de instalaciÃ³n, pero evaluar TA-Lib si se requiere mÃ¡s rendimiento.

### 3. SQLAlchemy ORM vs Core

| Aspecto | ORM | Core | RecomendaciÃ³n |
|---------|-----|------|---------------|
| Velocidad | Baseline | 2-5x mÃ¡s rÃ¡pido | âœ… Core |
| API | Alto nivel | Bajo nivel | âš ï¸ Core |
| Complejidad | Baja | Media | âš ï¸ Core |
| Mantenibilidad | Alta | Media | âš ï¸ Core |

**DecisiÃ³n:** Usar **bulk operations** de ORM (que internamente usa Core) para operaciones masivas.

## Cambios Priorizados por Impacto/Complejidad

### ðŸ”´ PRIORIDAD 1: Muy Alta Prioridad (Impacto Alto, Complejidad Baja)

#### Cambio 1.1: Reemplazar `iterrows()` con `itertuples()`

**UbicaciÃ³n:** [`finance_service.py:68`](finance_service.py:68)
**Impacto:** 50-70% mÃ¡s rÃ¡pido
**Complejidad:** Muy baja (1 lÃ­nea de cambio)

```python
# ANTES (lento)
for index, row in data.iterrows():
    date_val = index.date()
    # ...

# DESPUÃ‰S (rÃ¡pido)
for row in data.itertuples():
    date_val = row.Index.date()
    open_val = float(row.Open)
    high_val = float(row.High)
    low_val = float(row.Low)
    close_val = float(row.Close)
    volume_val = int(row.Volume)
```

**Beneficio:** Sin cambios en lÃ³gica, solo en cÃ³mo se acceden los datos.

---

#### Cambio 1.2: Usar `set()` para verificar duplicados

**UbicaciÃ³n:** [`finance_service.py:70`](finance_service.py:70)
**Impacto:** 90% mÃ¡s rÃ¡pido (elimina N+1 queries)
**Complejidad:** Baja (5-10 lÃ­neas de cambio)

```python
# ANTES (N+1 queries)
for index, row in data.iterrows():
    date_val = index.date()
    existing = Price.query.filter_by(ticker_id=ticker_obj.id, date=date_val).first()
    if not existing:
        # ...

# DESPUÃ‰S (1 query + set lookup O(1))
existing_dates = set(
    p.date for p in Price.query
    .with_entities(Price.date)
    .filter_by(ticker_id=ticker_obj.id)
    .all()
)

for row in data.itertuples():
    date_val = row.Index.date()
    if date_val not in existing_dates:
        # ...
```

**Beneficio:** Reduce de N queries a 1 query.

---

#### Cambio 1.3: Usar `bulk_save_objects()` para inserciones

**UbicaciÃ³n:** [`finance_service.py:82`](finance_service.py:82)
**Impacto:** 80-90% mÃ¡s rÃ¡pido
**Complejidad:** Baja (10-15 lÃ­neas de cambio)

```python
# ANTES (insert individual)
for index, row in data.iterrows():
    # ...
    price = Price(...)
    db.session.add(price)
    count += 1

db.session.commit()

# DESPUÃ‰S (bulk insert)
new_prices = []
for row in data.itertuples():
    date_val = row.Index.date()
    if date_val not in existing_dates:
        new_prices.append(Price(
            ticker_id=ticker_obj.id,
            date=date_val,
            open=float(row.Open),
            high=float(row.High),
            low=float(row.Low),
            close=float(row.Close),
            volume=int(row.Volume)
        ))

if new_prices:
    db.session.bulk_save_objects(new_prices)
    count = len(new_prices)
```

**Beneficio:** Una transacciÃ³n en lugar de N transacciones.

---

#### Cambio 1.4: Reducir delay entre tickers

**UbicaciÃ³n:** [`app.py:144`](app.py:144)
**Impacto:** 50% menos tiempo total
**Complejidad:** Muy baja (1 nÃºmero de cambio)

```python
# ANTES
delay_between_tickers = 1  # 1 segundo

# DESPUÃ‰S
delay_between_tickers = 0.3  # 0.3 segundos
```

**Beneficio:** Menos tiempo de espera sin riesgo de bloqueo (yfinance maneja bien 0.3s).

---

#### Cambio 1.5: Agregar Ã­ndice compuesto a Price

**UbicaciÃ³n:** [`database.py:25`](database.py:25)
**Impacto:** 30-50% mÃ¡s rÃ¡pido en consultas
**Complejidad:** Muy baja (1 lÃ­nea de cambio)

```python
# ANTES
__table_args__ = (
    db.UniqueConstraint('ticker_id', 'date', name='_ticker_date_uc'),
)

# DESPUÃ‰S
__table_args__ = (
    db.UniqueConstraint('ticker_id', 'date', name='_ticker_date_uc'),
    db.Index('idx_ticker_date', 'ticker_id', 'date'),
)
```

**Beneficio:** Consultas por ticker_id y date mucho mÃ¡s rÃ¡pidas.

---

### ðŸŸ¡ PRIORIDAD 2: Alta Prioridad (Impacto Alto/Medio, Complejidad Media)

#### Cambio 2.1: Usar `with_entities()` para cargar solo campos necesarios

**UbicaciÃ³n:** [`finance_service.py:95`](finance_service.py:95)
**Impacto:** 20-30% mÃ¡s rÃ¡pido, menos memoria
**Complejidad:** Baja (2-3 lÃ­neas de cambio)

```python
# ANTES
prices = Price.query.filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()

# DESPUÃ‰S
prices = Price.query.with_entities(
    Price.date, Price.open, Price.high,
    Price.low, Price.close, Price.volume
).filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()
```

**Beneficio:** Carga solo los campos necesarios, menos transferencia de datos.

---

#### Cambio 2.2: Usar `read_sql()` de pandas para carga directa

**UbicaciÃ³n:** [`finance_service.py:99`](finance_service.py:99)
**Impacto:** 40-50% mÃ¡s rÃ¡pido
**Complejidad:** Media (10-15 lÃ­neas de cambio)

```python
# ANTES
prices = Price.query.filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()
df = pd.DataFrame([{
    'date': p.date,
    'open': p.open,
    'high': p.high,
    'low': p.low,
    'close': p.close,
    'volume': p.volume
} for p in prices])

# DESPUÃ‰S
query = db.session.query(
    Price.date, Price.open, Price.high,
    Price.low, Price.close, Price.volume
).filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc())

df = pd.read_sql(query.statement, db.session.bind)
df.set_index('date', inplace=True)
```

**Beneficio:** Pandas hace la conversiÃ³n directamente desde SQL, mÃ¡s eficiente.

---

#### Cambio 2.3: Implementar cachÃ© simple con `@lru_cache`

**UbicaciÃ³n:** [`app.py:156`](app.py:156)
**Impacto:** 95% mÃ¡s rÃ¡pido en peticiones repetidas
**Complejidad:** Baja (10-15 lÃ­neas de cambio)

```python
from functools import lru_cache
import time

@lru_cache(maxsize=128)
def get_cached_signals(ticker_id, strategy, cache_key):
    ticker = Ticker.query.get(ticker_id)
    return FinanceService.get_signals(ticker, strategy=strategy)

@app.route('/api/scan', methods=['GET'])
def scan_tickers():
    strategy = request.args.get('strategy', 'rsi_macd')
    tickers = Ticker.query.all()

    # Cache key basado en tiempo (5 minutos de TTL)
    cache_key = int(time.time() // 300)

    signals = []
    for t in tickers:
        signal = get_cached_signals(t.id, strategy, cache_key)
        if signal:
            signals.append(signal)

    return jsonify(signals)
```

**Beneficio:** Respuestas casi instantÃ¡neas para peticiones repetidas.

---

### ðŸŸ¢ PRIORIDAD 3: Media Prioridad (Impacto Medio, Complejidad Media/Alta)

#### Cambio 3.1: Evaluar Polars para operaciones pesadas

**UbicaciÃ³n:** [`finance_service.py`](finance_service.py)
**Impacto:** 10-100x mÃ¡s rÃ¡pido en operaciones de datos
**Complejidad:** Media (requiere aprendizaje de API de Polars)

```python
# Ejemplo de uso de Polars (opcional, evaluar despuÃ©s de cambios bÃ¡sicos)
import polars as pl

# Convertir pandas DataFrame a polars para operaciones pesadas
df_pl = pl.from_pandas(df)

# Operaciones mÃ¡s rÃ¡pidas
rsi = df_pl.select(pl.col("close").rsi(14))
```

**Beneficio:** Para operaciones pesadas de datos, Polars es significativamente mÃ¡s rÃ¡pido.

**Nota:** Implementar solo si los cambios de Prioridad 1 y 2 no son suficientes.

---

#### Cambio 3.2: Procesamiento concurrente con ThreadPoolExecutor

**UbicaciÃ³n:** [`app.py:141`](app.py:141)
**Impacto:** 60-70% mÃ¡s rÃ¡pido para mÃºltiples tickers
**Complejidad:** Media (15-20 lÃ­neas de cambio)

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

@app.route('/api/refresh', methods=['POST'])
def refresh_data():
    tickers = Ticker.query.all()
    results = []

    # Limitar a 2-3 workers para evitar rate limiting
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_to_ticker = {
            executor.submit(FinanceService.sync_ticker_data, t): t
            for t in tickers
        }

        for future in as_completed(future_to_ticker):
            ticker = future_to_ticker[future]
            try:
                count = future.result()
                results.append({'symbol': ticker.symbol, 'new_records': count})
            except Exception as e:
                logger.error(f"Error syncing {ticker.symbol}: {e}")

    return jsonify(results)
```

**Beneficio:** Procesa mÃºltiples tickers en paralelo.

**Nota:** Implementar solo despuÃ©s de cambios de Prioridad 1 y 2.

---

## Orden de ImplementaciÃ³n Recomendado

### Fase 1: Cambios Inmediatos (1-2 horas)

1. âœ… Cambio 1.1: `iterrows()` â†’ `itertuples()`
2. âœ… Cambio 1.4: Reducir delay a 0.3s
3. âœ… Cambio 1.5: Agregar Ã­ndice a Price

**Beneficio esperado:** 40-60% mÃ¡s rÃ¡pido en sincronizaciÃ³n

---

### Fase 2: Cambios de Alto Impacto (2-3 horas)

4. âœ… Cambio 1.2: Usar `set()` para duplicados
5. âœ… Cambio 1.3: Usar `bulk_save_objects()`
6. âœ… Cambio 2.1: Usar `with_entities()`

**Beneficio esperado:** 80-90% mÃ¡s rÃ¡pido en sincronizaciÃ³n, 20-30% en seÃ±ales

---

### Fase 3: OptimizaciÃ³n Adicional (1-2 horas)

7. âœ… Cambio 2.2: Usar `read_sql()`
8. âœ… Cambio 2.3: Implementar cachÃ© simple

**Beneficio esperado:** 95% mÃ¡s rÃ¡pido en peticiones repetidas

---

### Fase 4: Opcional (si se requiere mÃ¡s rendimiento)

9. âš ï¸ Cambio 3.1: Evaluar Polars
10. âš ï¸ Cambio 3.2: Procesamiento concurrente

**Beneficio esperado:** Adicional 10-100x en operaciones pesadas, 60-70% en paralelo

---

## EstimaciÃ³n de Mejoras Totales

| MÃ©trica | Estado Actual | DespuÃ©s de Fase 1 | DespuÃ©s de Fase 2 | DespuÃ©s de Fase 3 |
|---------|--------------|-------------------|-------------------|-------------------|
| SincronizaciÃ³n 1 ticker | ~2-3s | ~1-1.5s | ~0.3-0.5s | ~0.2-0.3s |
| SincronizaciÃ³n 100 tickers | ~200-300s | ~80-120s | ~30-50s | ~30-50s |
| Endpoint /api/scan (1ra vez) | ~5-10s | ~4-8s | ~2-3s | ~1-2s |
| Endpoint /api/scan (cachÃ©) | ~5-10s | ~5-10s | ~5-10s | ~0.1-0.2s |
| InserciÃ³n 500 precios | ~5-8s | ~2-3s | ~0.5-1s | ~0.5-1s |

**Mejora total esperada despuÃ©s de Fase 3:** 80-95% mÃ¡s rÃ¡pido en la mayorÃ­a de operaciones.

---

## Diagrama de ImplementaciÃ³n

```mermaid
graph TD
    A[Estado Actual] --> B[Fase 1: Cambios Inmediatos]
    B --> C[Fase 2: Alto Impacto]
    C --> D[Fase 3: OptimizaciÃ³n Adicional]
    D --> E{Â¿Suficiente?}
    E -->|SÃ­| F[Fin]
    E -->|No| G[Fase 4: Opcional]

    B --> B1[iterrows to itertuples]
    B --> B2[Reducir delay]
    B --> B3[Ãndice Price]

    C --> C1[Set para duplicados]
    C --> C2[Bulk save objects]
    C --> C3[with entities]

    D --> D1[read sql]
    D --> D2[Cache lru]

    G --> G1[Evaluar Polars]
    G --> G2[Concurrent processing]

    style A fill:#f99,stroke:#333
    style F fill:#9f9,stroke:#333
    style B1 fill:#ff9,stroke:#333
    style C1 fill:#ff9,stroke:#333
    style D1 fill:#ff9,stroke:#333
```

## Archivos a Modificar

| Archivo | Cambios | LÃ­neas aprox. |
|---------|---------|---------------|
| [`finance_service.py`](finance_service.py) | 1.1, 1.2, 1.3, 2.1, 2.2 | ~30-40 |
| [`app.py`](app.py) | 1.4, 2.3, 3.2 | ~20-25 |
| [`database.py`](database.py) | 1.5 | ~1 |
| [`requirements.txt`](requirements.txt) | Opcional: polars | ~1 |

## Testing Plan

Antes de cada fase:
1. âœ… Backup de base de datos
2. âœ… Ejecutar `scripts/check_db.py` para estado inicial
3. âœ… Probar sincronizaciÃ³n de 5 tickers
4. âœ… Verificar endpoint `/api/scan`
5. âœ… Comparar tiempos antes/despuÃ©s

DespuÃ©s de cada fase:
1. âœ… Verificar que resultados sean idÃ©nticos
2. âœ… Documentar mejoras de rendimiento
3. âœ… Actualizar documentaciÃ³n

## Riesgos y Mitigaciones

| Riesgo | Probabilidad | Impacto | MitigaciÃ³n |
|--------|--------------|---------|------------|
| Bulk operations causa error de memoria | Baja | Medio | Implementar chunking si es necesario |
| Delay muy corto causa bloqueo API | Baja | Alto | Mantener monitoreo, ajustar si es necesario |
| CachÃ© devuelve datos desactualizados | Media | Bajo | TTL de 5 minutos es aceptable |
| Polars requiere cambios significativos | Alta | Medio | Solo implementar si es necesario |

---

**Aprobado por:** [Pendiente]
**Fecha de implementaciÃ³n:** [Pendiente]
