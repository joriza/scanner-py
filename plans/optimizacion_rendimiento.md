# Plan de Optimizaci√≥n de Rendimiento - Scanner Pro Python

**Fecha:** 27 de enero de 2026
**Estado:** Planificaci√≥n
**Enfoque:** Alto impacto, baja complejidad, librer√≠as m√°s r√°pidas

## Resumen Ejecutivo

Este documento detalla las optimizaciones propuestas para mejorar el rendimiento del proyecto Scanner Pro Python, priorizando cambios de alto impacto con baja complejidad y evaluando librer√≠as alternativas m√°s r√°pidas.

## Librer√≠as Alternativas Evaluadas

### 1. Polars vs pandas

| Aspecto | pandas | Polars | Recomendaci√≥n |
|---------|--------|--------|---------------|
| Velocidad | Baseline | 10-100x m√°s r√°pido | ‚úÖ Polars |
| API | Est√°ndar | Similar a pandas | ‚úÖ Polars |
| Complejidad | - | Baja | ‚úÖ Polars |
| Compatibilidad | - | Puede convertir a pandas | ‚úÖ Polars |
| Mantenimiento | Estable | Activo | ‚úÖ Polars |

**Decisi√≥n:** Reemplazar pandas con **Polars** donde sea posible sin complicar el c√≥digo.

### 2. TA-Lib vs pandas-ta

| Aspecto | pandas-ta | TA-Lib | Recomendaci√≥n |
|---------|-----------|--------|---------------|
| Velocidad | Baseline | 5-20x m√°s r√°pido | ‚úÖ TA-Lib |
| API | Python puro | Python wrapper sobre C | ‚ö†Ô∏è TA-Lib |
| Instalaci√≥n | pip simple | Requiere compilaci√≥n | ‚ö†Ô∏è TA-Lib |
| Complejidad | Baja | Media | ‚ö†Ô∏è TA-Lib |

**Decisi√≥n:** Mantener **pandas-ta** por simplicidad de instalaci√≥n, pero evaluar TA-Lib si se requiere m√°s rendimiento.

### 3. SQLAlchemy ORM vs Core

| Aspecto | ORM | Core | Recomendaci√≥n |
|---------|-----|------|---------------|
| Velocidad | Baseline | 2-5x m√°s r√°pido | ‚úÖ Core |
| API | Alto nivel | Bajo nivel | ‚ö†Ô∏è Core |
| Complejidad | Baja | Media | ‚ö†Ô∏è Core |
| Mantenibilidad | Alta | Media | ‚ö†Ô∏è Core |

**Decisi√≥n:** Usar **bulk operations** de ORM (que internamente usa Core) para operaciones masivas.

## Cambios Priorizados por Impacto/Complejidad

### üî¥ PRIORIDAD 1: Muy Alta Prioridad (Impacto Alto, Complejidad Baja)

#### Cambio 1.1: Reemplazar `iterrows()` con `itertuples()`

**Ubicaci√≥n:** [`finance_service.py:68`](finance_service.py:68)
**Impacto:** 50-70% m√°s r√°pido
**Complejidad:** Muy baja (1 l√≠nea de cambio)

```python
# ANTES (lento)
for index, row in data.iterrows():
    date_val = index.date()
    # ...

# DESPU√âS (r√°pido)
for row in data.itertuples():
    date_val = row.Index.date()
    open_val = float(row.Open)
    high_val = float(row.High)
    low_val = float(row.Low)
    close_val = float(row.Close)
    volume_val = int(row.Volume)
```

**Beneficio:** Sin cambios en l√≥gica, solo en c√≥mo se acceden los datos.

---

#### Cambio 1.2: Usar `set()` para verificar duplicados

**Ubicaci√≥n:** [`finance_service.py:70`](finance_service.py:70)
**Impacto:** 90% m√°s r√°pido (elimina N+1 queries)
**Complejidad:** Baja (5-10 l√≠neas de cambio)

```python
# ANTES (N+1 queries)
for index, row in data.iterrows():
    date_val = index.date()
    existing = Price.query.filter_by(ticker_id=ticker_obj.id, date=date_val).first()
    if not existing:
        # ...

# DESPU√âS (1 query + set lookup O(1))
existing_dates = set(
    p.date for p in Price.query
    .with_entities(Price.date)
    .filter_by(ticker_id=ticker_obj.id)
    .all()
)

for row in data.itertuples():
    date_val = row.Index.date()
    if date_val not in existing_dates:
        # ...
```

**Beneficio:** Reduce de N queries a 1 query.

---

#### Cambio 1.3: Usar `bulk_save_objects()` para inserciones

**Ubicaci√≥n:** [`finance_service.py:82`](finance_service.py:82)
**Impacto:** 80-90% m√°s r√°pido
**Complejidad:** Baja (10-15 l√≠neas de cambio)

```python
# ANTES (insert individual)
for index, row in data.iterrows():
    # ...
    price = Price(...)
    db.session.add(price)
    count += 1

db.session.commit()

# DESPU√âS (bulk insert)
new_prices = []
for row in data.itertuples():
    date_val = row.Index.date()
    if date_val not in existing_dates:
        new_prices.append(Price(
            ticker_id=ticker_obj.id,
            date=date_val,
            open=float(row.Open),
            high=float(row.High),
            low=float(row.Low),
            close=float(row.Close),
            volume=int(row.Volume)
        ))

if new_prices:
    db.session.bulk_save_objects(new_prices)
    count = len(new_prices)
```

**Beneficio:** Una transacci√≥n en lugar de N transacciones.

---

#### Cambio 1.4: Reducir delay entre tickers

**Ubicaci√≥n:** [`app.py:144`](app.py:144)
**Impacto:** 50% menos tiempo total
**Complejidad:** Muy baja (1 n√∫mero de cambio)

```python
# ANTES
delay_between_tickers = 1  # 1 segundo

# DESPU√âS
delay_between_tickers = 0.3  # 0.3 segundos
```

**Beneficio:** Menos tiempo de espera sin riesgo de bloqueo (yfinance maneja bien 0.3s).

---

#### Cambio 1.5: Agregar √≠ndice compuesto a Price

**Ubicaci√≥n:** [`database.py:25`](database.py:25)
**Impacto:** 30-50% m√°s r√°pido en consultas
**Complejidad:** Muy baja (1 l√≠nea de cambio)

```python
# ANTES
__table_args__ = (
    db.UniqueConstraint('ticker_id', 'date', name='_ticker_date_uc'),
)

# DESPU√âS
__table_args__ = (
    db.UniqueConstraint('ticker_id', 'date', name='_ticker_date_uc'),
    db.Index('idx_ticker_date', 'ticker_id', 'date'),
)
```

**Beneficio:** Consultas por ticker_id y date mucho m√°s r√°pidas.

---

### üü° PRIORIDAD 2: Alta Prioridad (Impacto Alto/Medio, Complejidad Media)

#### Cambio 2.1: Usar `with_entities()` para cargar solo campos necesarios

**Ubicaci√≥n:** [`finance_service.py:95`](finance_service.py:95)
**Impacto:** 20-30% m√°s r√°pido, menos memoria
**Complejidad:** Baja (2-3 l√≠neas de cambio)

```python
# ANTES
prices = Price.query.filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()

# DESPU√âS
prices = Price.query.with_entities(
    Price.date, Price.open, Price.high,
    Price.low, Price.close, Price.volume
).filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()
```

**Beneficio:** Carga solo los campos necesarios, menos transferencia de datos.

---

#### Cambio 2.2: Usar `read_sql()` de pandas para carga directa

**Ubicaci√≥n:** [`finance_service.py:99`](finance_service.py:99)
**Impacto:** 40-50% m√°s r√°pido
**Complejidad:** Media (10-15 l√≠neas de cambio)

```python
# ANTES
prices = Price.query.filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc()).all()
df = pd.DataFrame([{
    'date': p.date,
    'open': p.open,
    'high': p.high,
    'low': p.low,
    'close': p.close,
    'volume': p.volume
} for p in prices])

# DESPU√âS
query = db.session.query(
    Price.date, Price.open, Price.high,
    Price.low, Price.close, Price.volume
).filter_by(ticker_id=ticker_obj.id).order_by(Price.date.asc())

df = pd.read_sql(query.statement, db.session.bind)
df.set_index('date', inplace=True)
```

**Beneficio:** Pandas hace la conversi√≥n directamente desde SQL, m√°s eficiente.

---

#### Cambio 2.3: Implementar cach√© simple con `@lru_cache`

**Ubicaci√≥n:** [`app.py:156`](app.py:156)
**Impacto:** 95% m√°s r√°pido en peticiones repetidas
**Complejidad:** Baja (10-15 l√≠neas de cambio)

```python
from functools import lru_cache
import time

@lru_cache(maxsize=128)
def get_cached_signals(ticker_id, strategy, cache_key):
    ticker = Ticker.query.get(ticker_id)
    return FinanceService.get_signals(ticker, strategy=strategy)

@app.route('/api/scan', methods=['GET'])
def scan_tickers():
    strategy = request.args.get('strategy', 'rsi_macd')
    tickers = Ticker.query.all()

    # Cache key basado en tiempo (5 minutos de TTL)
    cache_key = int(time.time() // 300)

    signals = []
    for t in tickers:
        signal = get_cached_signals(t.id, strategy, cache_key)
        if signal:
            signals.append(signal)

    return jsonify(signals)
```

**Beneficio:** Respuestas casi instant√°neas para peticiones repetidas.

---

### üü¢ PRIORIDAD 3: Media Prioridad (Impacto Medio, Complejidad Media/Alta)

#### Cambio 3.1: Evaluar Polars para operaciones pesadas

**Ubicaci√≥n:** [`finance_service.py`](finance_service.py)
**Impacto:** 10-100x m√°s r√°pido en operaciones de datos
**Complejidad:** Media (requiere aprendizaje de API de Polars)

```python
# Ejemplo de uso de Polars (opcional, evaluar despu√©s de cambios b√°sicos)
import polars as pl

# Convertir pandas DataFrame a polars para operaciones pesadas
df_pl = pl.from_pandas(df)

# Operaciones m√°s r√°pidas
rsi = df_pl.select(pl.col("close").rsi(14))
```

**Beneficio:** Para operaciones pesadas de datos, Polars es significativamente m√°s r√°pido.

**Nota:** Implementar solo si los cambios de Prioridad 1 y 2 no son suficientes.

---

#### Cambio 3.2: Procesamiento concurrente con ThreadPoolExecutor

**Ubicaci√≥n:** [`app.py:141`](app.py:141)
**Impacto:** 60-70% m√°s r√°pido para m√∫ltiples tickers
**Complejidad:** Media (15-20 l√≠neas de cambio)

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

@app.route('/api/refresh', methods=['POST'])
def refresh_data():
    tickers = Ticker.query.all()
    results = []

    # Limitar a 2-3 workers para evitar rate limiting
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_to_ticker = {
            executor.submit(FinanceService.sync_ticker_data, t): t
            for t in tickers
        }

        for future in as_completed(future_to_ticker):
            ticker = future_to_ticker[future]
            try:
                count = future.result()
                results.append({'symbol': ticker.symbol, 'new_records': count})
            except Exception as e:
                logger.error(f"Error syncing {ticker.symbol}: {e}")

    return jsonify(results)
```

**Beneficio:** Procesa m√∫ltiples tickers en paralelo.

**Nota:** Implementar solo despu√©s de cambios de Prioridad 1 y 2.

---

## Orden de Implementaci√≥n Recomendado

### Fase 1: Cambios Inmediatos (1-2 horas)

1. ‚úÖ Cambio 1.1: `iterrows()` ‚Üí `itertuples()`
2. ‚úÖ Cambio 1.4: Reducir delay a 0.3s
3. ‚úÖ Cambio 1.5: Agregar √≠ndice a Price

**Beneficio esperado:** 40-60% m√°s r√°pido en sincronizaci√≥n

---

### Fase 2: Cambios de Alto Impacto (2-3 horas)

4. ‚úÖ Cambio 1.2: Usar `set()` para duplicados
5. ‚úÖ Cambio 1.3: Usar `bulk_save_objects()`
6. ‚úÖ Cambio 2.1: Usar `with_entities()`

**Beneficio esperado:** 80-90% m√°s r√°pido en sincronizaci√≥n, 20-30% en se√±ales

---

### Fase 3: Optimizaci√≥n Adicional (1-2 horas)

7. ‚úÖ Cambio 2.2: Usar `read_sql()`
8. ‚úÖ Cambio 2.3: Implementar cach√© simple

**Beneficio esperado:** 95% m√°s r√°pido en peticiones repetidas

---

### Fase 4: Opcional (si se requiere m√°s rendimiento)

9. ‚ö†Ô∏è Cambio 3.1: Evaluar Polars
10. ‚ö†Ô∏è Cambio 3.2: Procesamiento concurrente

**Beneficio esperado:** Adicional 10-100x en operaciones pesadas, 60-70% en paralelo

---

## Estimaci√≥n de Mejoras Totales

| M√©trica | Estado Actual | Despu√©s de Fase 1 | Despu√©s de Fase 2 | Despu√©s de Fase 3 |
|---------|--------------|-------------------|-------------------|-------------------|
| Sincronizaci√≥n 1 ticker | ~2-3s | ~1-1.5s | ~0.3-0.5s | ~0.2-0.3s |
| Sincronizaci√≥n 100 tickers | ~200-300s | ~80-120s | ~30-50s | ~30-50s |
| Endpoint /api/scan (1ra vez) | ~5-10s | ~4-8s | ~2-3s | ~1-2s |
| Endpoint /api/scan (cach√©) | ~5-10s | ~5-10s | ~5-10s | ~0.1-0.2s |
| Inserci√≥n 500 precios | ~5-8s | ~2-3s | ~0.5-1s | ~0.5-1s |

**Mejora total esperada despu√©s de Fase 3:** 80-95% m√°s r√°pido en la mayor√≠a de operaciones.

---

## Posibles Mejoras Adicionales

### 1. Benchmarking Previa

**Descripci√≥n:** Antes de implementar optimizaciones, realizar mediciones de referencia para establecer una l√≠nea base real.

**Implementaci√≥n:**
```python
# scripts/benchmark.py
import time
from app import app, db
from models import Ticker, Price

def benchmark_sync_ticker(symbol):
    ticker = Ticker.query.filter_by(symbol=symbol).first()
    if not ticker:
        return None
    
    start = time.time()
    count = FinanceService.sync_ticker_data(ticker)
    elapsed = time.time() - start
    
    return {
        'symbol': symbol,
        'records_added': count,
        'time_seconds': elapsed
    }

def run_benchmark():
    tickers = Ticker.query.limit(10).all()
    results = []
    
    for t in tickers:
        result = benchmark_sync_ticker(t.symbol)
        if result:
            results.append(result)
    
    print("\n=== BENCHMARK DE REFERENCIA ===")
    for r in results:
        print(f"{r['symbol']}: {r['time_seconds']:.2f}s - {r['records_added']} registros")
    
    avg_time = sum(r['time_seconds'] for r in results) / len(results)
    print(f"\nTiempo promedio: {avg_time:.2f}s")
```

**Beneficio:** Permite validar las mejoras de rendimiento con datos reales.

---

### 2. Consideraci√≥n de Impacto en Memoria

**Descripci√≥n:** Evaluar el impacto de las operaciones bulk en el uso de memoria, especialmente con grandes vol√∫menes de datos.

**Implementaci√≥n:**
```python
# Chunking para operaciones bulk
def sync_ticker_data_with_chunking(ticker_obj, data, chunk_size=1000):
    existing_dates = set(
        p.date for p in Price.query
        .with_entities(Price.date)
        .filter_by(ticker_id=ticker_obj.id)
        .all()
    )
    
    new_prices = []
    count = 0
    
    for row in data.itertuples():
        date_val = row.Index.date()
        if date_val not in existing_dates:
            new_prices.append(Price(
                ticker_id=ticker_obj.id,
                date=date_val,
                open=float(row.Open),
                high=float(row.High),
                low=float(row.Low),
                close=float(row.Close),
                volume=int(row.Volume)
            ))
            count += 1
            
            # Commit en chunks para evitar memoria alta
            if len(new_prices) >= chunk_size:
                db.session.bulk_save_objects(new_prices)
                db.session.commit()
                new_prices = []
    
    # Commit restante
    if new_prices:
        db.session.bulk_save_objects(new_prices)
        db.session.commit()
    
    return count
```

**Beneficio:** Evita picos de memoria altos con grandes vol√∫menes de datos.

---

### 3. Evaluaci√≥n de Impacto en Concurrencia del Cach√©

**Descripci√≥n:** Analizar c√≥mo el cach√© LRU afecta la concurrencia en entornos multi-usuario.

**Implementaci√≥n:**
```python
# Opci√≥n A: Usar cach√© con invalidaci√≥n por ticker
from functools import lru_cache
import time

@lru_cache(maxsize=128)
def get_cached_signals(ticker_id, strategy, cache_key):
    ticker = Ticker.query.get(ticker_id)
    return FinanceService.get_signals(ticker, strategy=strategy)

@app.route('/api/scan', methods=['GET'])
def scan_tickers():
    strategy = request.args.get('strategy', 'rsi_macd')
    tickers = Ticker.query.all()
    
    signals = []
    for t in tickers:
        # Cache key basado en ticker_id y estrategia (no en tiempo)
        cache_key = f"{t.id}_{strategy}"
        signal = get_cached_signals(t.id, strategy, cache_key)
        if signal:
            signals.append(signal)
    
    return jsonify(signals)

# Opci√≥n B: Usar cach√© con invalidaci√≥n manual
from flask_caching import Cache
cache = Cache(app, config={'CACHE_TYPE': 'simple'})

@app.route('/api/scan', methods=['GET'])
@cache.cached(timeout=300, key_prefix=lambda: request.url)
def scan_tickers():
    # ... c√≥digo original ...
```

**Beneficio:** Mejor control sobre el comportamiento del cach√© en entornos concurrentes.

---

### 4. Monitoreo de Rendimiento en Producci√≥n

**Descripci√≥n:** Implementar m√©tricas de rendimiento para monitorear el impacto de las optimizaciones.

**Implementaci√≥n:**
```python
# middleware.py
import time
from flask import request, g

@app.before_request
def start_timer():
    g.start_time = time.time()

@app.after_request
def log_request(response):
    if hasattr(g, 'start_time'):
        elapsed = time.time() - g.start_time
        logger.info(f"{request.method} {request.path} - {elapsed:.3f}s")
        response.headers['X-Response-Time'] = f"{elapsed:.3f}s"
    return response

# metrics.py
from prometheus_client import Counter, Histogram

request_count = Counter('http_requests_total', 'Total HTTP requests')
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration')

@app.before_request
def increment_request_count():
    request_count.inc()

@app.after_request
def record_request_duration(response):
    if hasattr(g, 'start_time'):
        duration = time.time() - g.start_time
        request_duration.observe(duration)
    return response
```

**Beneficio:** Permite identificar cuellos de botella y medir el impacto de optimizaciones.

---

### 5. Revisi√≥n de Logs y Depuraci√≥n

**Descripci√≥n:** Agregar logs detallados para identificar cuellos de botella espec√≠ficos.

**Implementaci√≥n:**
```python
# En finance_service.py
def sync_ticker_data(ticker_obj):
    start_time = time.time()
    logger.info(f"Comenzando sincronizaci√≥n de {ticker_obj.symbol}")
    
    try:
        # ... c√≥digo existente ...
        
        elapsed = time.time() - start_time
        logger.info(f"  {ticker_obj.symbol}: {count} nuevos registros agregados en {elapsed:.2f}s")
        
        return count
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"  {ticker_obj.symbol}: Error en {elapsed:.2f}s - {e}")
        raise
```

**Beneficio:** Facilita la identificaci√≥n de problemas y medici√≥n precisa de mejoras.

---

## Diagrama de Implementaci√≥n

```mermaid
graph TD
    A[Estado Actual] --> B[Fase 1: Cambios Inmediatos]
    B --> C[Fase 2: Alto Impacto]
    C --> D[Fase 3: Optimizaci√≥n Adicional]
    D --> E{¬øSuficiente?}
    E -->|S√≠| F[Fin]
    E -->|No| G[Fase 4: Opcional]

    B --> B1[iterrows to itertuples]
    B --> B2[Reducir delay]
    B --> B3[√çndice Price]

    C --> C1[Set para duplicados]
    C --> C2[Bulk save objects]
    C --> C3[with entities]

    D --> D1[read sql]
    D --> D2[Cache lru]

    G --> G1[Evaluar Polars]
    G --> G2[Concurrent processing]

    style A fill:#f99,stroke:#333
    style F fill:#9f9,stroke:#333
    style B1 fill:#ff9,stroke:#333
    style C1 fill:#ff9,stroke:#333
    style D1 fill:#ff9,stroke:#333
```

## Archivos a Modificar

### Archivos Principales

| Archivo | Cambios | L√≠neas aprox. |
|---------|---------|---------------|
| [`finance_service.py`](finance_service.py) | 1.1, 1.2, 1.3, 2.1, 2.2 | ~30-40 |
| [`app.py`](app.py) | 1.4, 2.3, 3.2 | ~20-25 |
| [`database.py`](database.py) | 1.5 | ~1 |
| [`requirements.txt`](requirements.txt) | Opcional: polars | ~1 |

### Archivos para Mejoras Adicionales

| Archivo | Prop√≥sito | L√≠neas aprox. |
|---------|-----------|---------------|
| [`scripts/benchmark.py`](scripts/benchmark.py) | Benchmarking previo | ~30-40 |
| [`middleware.py`](middleware.py) | Middleware de monitoreo | ~20-30 |
| [`metrics.py`](metrics.py) | M√©tricas de rendimiento | ~30-40 |
| [`scripts/sync_with_chunking.py`](scripts/sync_with_chunking.py) | Sincronizaci√≥n con chunking | ~40-50 |

## Testing Plan

Antes de cada fase:
1. ‚úÖ Backup de base de datos
2. ‚úÖ Ejecutar `scripts/check_db.py` para estado inicial
3. ‚úÖ Probar sincronizaci√≥n de 5 tickers
4. ‚úÖ Verificar endpoint `/api/scan`
5. ‚úÖ Comparar tiempos antes/despu√©s

Despu√©s de cada fase:
1. ‚úÖ Verificar que resultados sean id√©nticos
2. ‚úÖ Documentar mejoras de rendimiento
3. ‚úÖ Actualizar documentaci√≥n

## Riesgos y Mitigaciones

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Bulk operations causa error de memoria | Baja | Medio | Implementar chunking si es necesario |
| Delay muy corto causa bloqueo API | Baja | Alto | Mantener monitoreo, ajustar si es necesario |
| Cach√© devuelve datos desactualizados | Media | Bajo | TTL de 5 minutos es aceptable |
| Polars requiere cambios significativos | Alta | Medio | Solo implementar si es necesario |

---

**Aprobado por:** [Pendiente]
**Fecha de implementaci√≥n:** [Pendiente]
